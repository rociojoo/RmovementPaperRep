---
title: "Survey on the R tracking packages: part I"
author: "Rocio Joo and Matthew E. Boone"
date: "October 14, 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
```{r, echo=FALSE, message=F}
knitr::opts_chunk$set(echo = TRUE,fig.width=16, fig.height=10)
library(tidyverse)
library(ggplot2)
library(ggrepel)
library(dplyr)
library(reshape)
library(RColorBrewer)
library(kableExtra)
# setwd('~/r_programs/RmovementPaperRep/SurveyPosts')
data <- read.csv("survey_responses_20190130.csv",stringsAsFactors = FALSE) 
data_all <- data %>% 
  filter(completion == 100)

packages <- read_csv('packages_survey_names.csv')
```
<!-- ## A large amount of R packages for movement and the need for a survey -->

In a recent post <!--- Need link -->, we introduced the pre-print of our review of R packages for movement. R is one of the most used programming softwares to process, visualize and analyze data from tracking devices. The large amount of existing packages makes it difficult to keep track of the spectrum of choices. Our review aimed at an objective introduction to the packages organized by the type of processing or analyzing they focused on, and to provide feedback to developers from a user perspective. For the second objective, we elaborated a survey for package users regarding 

    1. How popular those packages are; 
    2. How well documented they are; 
    3. How relevant they are for users. 
    
<!-- Those were the three questions that we asked about the packages, plus one about the level as an R user of the survey participant. We will go to the details of each question later in this post.  -->

In the review we showed results regarding package documentation. In this post you'll find the complete results of the survey. 

## Packages included in the survey

In theory, any package could be potentially useful for movement analysis; either a time series package, a spatial analysis one or even *ggplot2* to make more beautiful graphics! For the [review](https://arxiv.org/abs/1901.05935), we considered only what we referred to as tracking packages. Tracking packages were those created to either analyze tracking data (i.e. (x, y, t)) or to transform data from tagging devices into proper tracking data. For instance, a package that would use accelerometer, gyroscope and magnetometer data to reconstruct an animal's trajectory via path integration, thus transforming those data into an (x,y,t) format, would fit into the definition. But a package analyzing accelerometry series to detect changes in behavior would not fit. 

For this survey, we added packages that, though not analyzing tracking data, they were created to process or analyze data extracted from tracking devices in other formats (e.g. *accelerometry* for accelerometry data, *diveMove* for time-depth recorders or *pathtrackr* for video tracking data). A couple of packages that, were finally discarded from the review because of either being in early stages of development (*movement*) or because of being removed from CRAN due to unfixed problems (*sigloc*), were included in the survey. 
<!-- For this survey, we considered only packages created with the main purpose of 1) processing or analyzing data obtained from devices used to track organisms, or 2) analyze tracking data in general. Tracking data would have space and time components, i.e. at least (x,y,t), or assuming t as a general index in regular steps.  -->

A total of 72 packages were included in this survey: *acc*, *accelerometry*, *adehabitatHR*, *adehabitatHS*, *adehabitatLT*, *amt*, *animalTrack*, *anipaths*, *argosfilter*, *argosTrack*, *BayesianAnimalTracker*, *BBMM*, *bcpa*, *bsam*, *caribou*, *crawl*, *ctmcmove*, *ctmm*, *diveMove*, *drtracker*, *EMbC*, *feedR*, *FLightR*, *GeoLight*, *GGIR*, *hab*, *HMMoce*, *Kftrack*, *m2b*, *marcher*, *migrateR*, *mkde*, *momentuHMM*, *move*, *moveHMM*, *movement*, *movementAnalysis*, *moveNT*, *moveVis*, *moveWindSpeed*, *nparACT*, *pathtrackr*, *pawacc*, *PhysicalActivity*, *probgls*, *rbl*, *recurse*, *rhr*, *rpostgisLT*, *rsMove*, *SDLfilter*, *SGAT/TripEstimation*, *sigloc*, *SimilarityMeasures*, *SiMRiv*, *smam*, *SwimR*, *T-LoCoH*, *telemetr*, *trackdem*, *trackeR*, *Trackit*, *TrackReconstruction*, *TrajDataMining*, *trajectories*, *trip*, *TwGeos/BAStag*, *TwilightFree*, *Ukfsst/kfsst*, *VTrack* and *wildlifeDI*.

Packages from any public repository (e.g. CRAN, GitHub, R-forge) were included in the survey. Packages created for eye, computer-mouse or fishing vessel movement were not considered here (but you are welcome to make your own survey about them!). 

Note: *trajr* was added to the survey in a late stage, but because of that, and the fact that it got only one response, we filtered it out of the analysis.

## Participation in the survey

The survey was designed to be completely anonymous, meaning that we had no way to know who participated and not even the date of participation was saved. There was no previous selection of the participants and no probabilistic sampling was involved. The survey was advertised by Twitter, mailing lists (r-sig-geo and r-sig-ecology), individual emails to researchers and the [mablab website](http://mablab.org). 

The survey got exemption from the Institutional Review Board at University of Florida. IRB02 Office, Box 112250, University of Florida, Gainesville, FL 32611-2250.

`r data %>% filter(!is.na(completion)) %>% nrow()` people participated in the survey, and `r data %>% filter(completion==100) %>% nrow()` answered all four questions. To answer all questions the participant had to have tried at least one of the packages. In the following sections, we analyze only completed surveys.


## The questions

### User level

Let's see first the level of use in R of the participants. The options were:

* Beginner: You only use existing packages and occasionally write some lines of code.
* Intermediate: You use existing packages but you also write and optimize your own functions.
* Advanced: You commonly use version control or contribute to develop packages.


```{r User experience, echo=FALSE, message=F, fig.width=14, fig.height=10, fig.cap=paste("Figure 1. Level of R use")}
#knitr::opts_chunk$set(echo = TRUE,fig.width=16, fig.height=8)

### 4. R user
data_question <- data_all[,grep("q4", colnames(data_all))]
categories <- c("Beginner","Intermediate","Advanced")
data_question <- factor(unlist(lapply(strsplit(data_question,'\\:'),'[[',1)),levels=categories)
use_counts <- as.numeric(table(data_question))
prop <- round(as.numeric(prop.table(table(data_question)))*100,1)
use_counts<-data.frame(levels=categories,total=use_counts)
use_counts$levels <- factor(as.character(use_counts$levels),levels=categories)
ggplot(data=use_counts,aes(x=levels,y=total)) + geom_bar(stat="identity", position=position_dodge()) +
  geom_text(aes(label=total), size=6.0, vjust=1.5, hjust=1.2, color="white",
            position = position_dodge(0.9)) +  theme_classic() +
            theme(axis.text.y = element_text( hjust = 1,vjust=0.5),
                  axis.title =element_text(size=24),
                  axis.text =element_text(size=15))

```
<!---[caption comments](/raccoons/img/2019-01-13_skull-dentition_S.jpg
#"caption hover over")<span class="image-caption">caption comments</span>
-->
Most participants considered themselves in an intermediate level (`r prop[2]` %), meaning that they could write functions in R. Some others were beginners (`r prop[1]` %) and advanced (`r prop[3]` %) R users. 

### Package use

The first question about package use was: How often do you use each of these packages? (Never, Rarely, Sometimes, Often)

The bar graphics below show that most packages were unknown (or at least had never been used) by the survey participants. The adehabitat packages (HR, LT and HS) were the most used packages. These packages provide a collection of tools to estimate home range, perform simple trajectory analysis and analyze habitat selection, respectively. On the bottom of the graphic, *smam* (for animal movement models), *PhysicalActivity*, *nparACT*, *GGIR* (these three for accelerometry data on human patients) and *feedr* (to handle radio telemetry data) had no users among the participants. For that reason, those 5 packages will not appear in the analysis of the next questions. 

```{r relevance, echo=FALSE, message=F, fig.width=16, fig.height=15, fig.cap="Figure 2. Package use"}
data_question <- data_all[,grep("q1", colnames(data_all))]
colnames(data_question) <- t(packages)
# I'm dropping trajr that was added in the end and only got 1 response
data_question <- data_question[,-grep("trajr",colnames(data_question))]
packages_new <- packages[-which(packages=="trajr"),]

categories <- c("Never","Rarely","Sometimes","Often")
use_counts <- t(sapply(1:ncol(data_question),function(x){
  data_line <- factor(data_question[,x],levels=categories)
  count_p_use <- as.numeric(table(data_line))
  return(count_p_use)
}))
use_counts<-data.frame(use_counts)
colnames(use_counts) <- categories
rownames(use_counts) <- t(packages_new)
use_counts$package<-row.names(use_counts)

df1<-melt(use_counts,id.vars='package',variable_name='response')
g<-unlist(by(df1, df1$package, function(x) sum(x$value[x$response!='Never'])))

df1$package<-factor(df1$package,levels=names(sort(g,decreasing=F)))
color.pallete<-brewer.pal(5,'YlGnBu')
color.pallete[1]<-'lightgray'
 ggplot(data=df1) + geom_col(aes(x=package, y=value,fill=response)) + ylab('count') +coord_flip() + scale_fill_manual(values=color.pallete) + theme_classic()+
  theme(axis.text.y = element_text( hjust = 1,vjust=0.5),
        axis.title =element_text(size=24),
        axis.text =element_text(size=15),
        legend.text=element_text(size=16),
        legend.title=element_blank())

```
<!---<span class="image-caption">caption comments</span>
-->

If you want to check the numbers for your favorite package, the complete table is [here](make another page for just the table)

<!--
```{r relevance table, echo=FALSE, message=F}
kable(use_counts[,1:length(categories)]) %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"))
```
-->

### Package documentation

Without proper user testing and peer editing, package documentation can lead to large gaps of understanding and limited usefulness of the package. If functions and workflows are not expressly defined, a package’s capacity to help users is undermined. 

In this survey we asked the participants how helpful was the documentation provided for each of the packages they stated to use. Documentation includes what is contained in the manual and help pages, vignettes, published manuscripts, and other material about the package provided by the authors. The participants had to answer using one of the following options:

* Not enough: "It's not enough to let me know how to do what I need"
* Basic: "It's enough to let me get started with simple use of the functions but not to go further (e.g. use all arguments in the functions, or put extra variables)"
* Good: "I did everything I wanted and needed to do with it"
* Excellent: "I ended up doing even more than what I planned because of the excellent information in the documentation"
* Don't remember: "I honestly can't remember…"

```{r documentation, echo=FALSE, message=F, fig.width=16, fig.height=15, fig.cap="Figure 3. Bar plots of absolute frequency of each category of package documentation"}
data_question <- data_all[,grep("q2", colnames(data_all))]
colnames(data_question) <- t(packages)
# I'm dropping trajr that was added in the end and only got 1 response
data_question <- data_question[,-grep("trajr",colnames(data_question))]
packages_new <- packages[-which(packages=="trajr"),]

categories <- c("Not enough","Basic","Good","Excellent", "Don't remember")
use_counts <- t(sapply(1:ncol(data_question),function(x){
  data_line <- factor(data_question[,x],levels=categories)
  count_p_use <- as.numeric(table(data_line))
  return(count_p_use)
}))
use_counts<-data.frame(use_counts)
colnames(use_counts) <- categories
rownames(use_counts) <- t(packages_new)
total_package <- rowSums(use_counts)
use_counts$package<-row.names(use_counts)
use_counts <- use_counts[total_package > 0,]

df1<-melt(use_counts,id.vars='package',variable_name='response')

g<-unlist(by(df1, df1$package, function(x) sum(x$value)))

color.pallete<-brewer.pal(5,'YlGnBu')
color.pallete[1]<-'lightgray'
df1$package<-factor(df1$package,levels=names(sort(g,decreasing=F)))
df1$response<-factor(df1$response,levels=rev(c('Excellent','Good','Basic','Not enough',"Don't remember")))
plot_use <- ggplot(data=df1) + geom_col(aes(x=package, y=value,fill=response)) + ylab('count') +coord_flip() + 
scale_fill_manual(values=color.pallete) +theme_classic() +
  theme(axis.text.y = element_text( hjust = 1,vjust=0.5),
        axis.title =element_text(size=24),
        axis.text =element_text(size=15),
        legend.text=element_text(size=16),
        legend.title=element_blank())
plot_use
```
<!---[caption comments](/raccoons/img/2019-01-13_skull-dentition_S.jpg
#"caption hover over")<span class="image-caption">caption comments</span>
-->

```{r useage counts,  echo=FALSE}
df2<-use_counts[,c("Not enough","Basic","Good","Excellent")]

use_per<-df2/apply(df2,1,sum)*100
use_per$good_excellent<-signif(apply(use_per[,c('Good','Excellent')],1,sum),4)
use_per$counts<-apply(df2[,c('Good','Excellent')],1,sum)
```
Remember that participants could only give their opinion on documentation regarding the packages they had used. Hence, the packages with many users got many documentation answers (Fig. 1 and 3). Figure 4 allows for a closer look at the proportion of type of response for each package. 

To identify some packages with remarkably good documentation, let's first only consider those packages with at least 10 responses on the quality of documentation (regardless of the "Don't remember"). These are 27 (you can see the table of responses [here](hyperlink)). Among them, *momentuHMM* had more than 50% of the responses (`r round(use_per['momentuHMM','Excellent'],2)`%; `r use_counts['momentuHMM','Excellent']`) as "excellent documentation", meaning that the documentation was so good that thanks to it, more than half of its users discovered additional features of the package and were able to do more analyses than what they initially planned. Moreover, 11 packages had more than 75% of the responses as either "good" or "excellent": *momentuHMM* (`r use_per['momentuHMM','good_excellent']`%; `r use_per['momentuHMM','counts']`), *moveHMM* (`r use_per['moveHMM','good_excellent']`%; `r use_per['moveHMM','counts']`), *adehabitatLT* (`r use_per['adehabitatLT','good_excellent']`%; `r use_per['adehabitatLT','counts']`), *adehabitatHS* (`r use_per['adehabitatHS','good_excellent']`%; `r use_per['adehabitatHS','counts']`), *adehabitatHR* (`r use_per['adehabitatHR','good_excellent']`%; `r use_per['adehabitatHR','counts']`), *EMbC* (`r use_per['EMbC','good_excellent']`%; `r use_per['EMbC','counts']`), *wildlifeDI* (`r use_per['wildlifeDI','good_excellent']`%; `r use_per['wildlifeDI','counts']`), *ctmm* (`r use_per['ctmm','good_excellent']`%; `r use_per['ctmm','counts']`), *GeoLight* (`r use_per['GeoLight','good_excellent']`%; `r use_per['GeoLight','counts']`), *move* (`r use_per['move','good_excellent']`%; `r use_per['move','counts']`), *recurse* (`r use_per['recurse','good_excellent']`%; `r use_per['recurse','counts']`). The two leading packages, *momentuHMM* and *moveHMM*, focus on the use of Hidden Markov models which allow identifying different patterns of behavior called states. 


```{r documentation percentage, echo=FALSE, message=F, fig.width=16, fig.height=10, fig.cap="Figure 4. Bar plots of relative frequency of each category of package documentation (for packages with more than 5 users)"}

package_levels<-row.names(use_per[order(use_per$good_excellent,use_per$Excellent, use_per$Good),])

use_per2<-use_per
use_per2[is.na(use_per2)]<-0
use_per2$package<-row.names(use_per2)
use_per2<-subset(use_per2,counts>5)
df1<-melt(subset(use_per2,select=-c(good_excellent,counts)),id.vars='package',variable_name='response')
df1$package<-factor(df1$package, levels=package_levels)


#df1$package<-factor(df1$package,levels=names(sort(g,decreasing=F)))
df1$response<-factor(df1$response,levels=rev(c('Excellent','Good','Basic','Not enough')))
color.pallete<-brewer.pal(4,'YlGnBu')
color.pallete[1:2]<-c('lightgray','darkgray')
#color.pallete[1]<-c('lightgray')
ggplot(data=df1) + geom_col(aes(x=package, y=value,fill=response))  + ylab('percentage') +coord_flip() + 
  scale_fill_manual(values=color.pallete) +theme_classic() +
  theme(axis.text.y = element_text( hjust = 1,vjust=0.5),
        axis.title =element_text(size=24),
        axis.text =element_text(size=15),
        legend.text=element_text(size=16),
        legend.title=element_blank())
```
<!---[caption comments](/raccoons/img/2019-01-13_skull-dentition_S.jpg
#"caption hover over")<span class="image-caption">caption comments</span>
-->
If you want to check the numbers for your favorite package, the complete table is [here](make another page for just the table)

<!--
```{r documentation table, echo=FALSE, message=F}
kable(use_counts[,1:length(categories)])%>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"))
```
-->
<!---[caption comments](/raccoons/img/2019-01-13_skull-dentition_S.jpg
#"caption hover over")<span class="image-caption">caption comments</span>
-->
## Package Revelance

Participants were asked how relevant was each of the packages they use for their work. They had to answer using one of the following options:

* Not relevant: "I tried the package but really didn't find it a good use for my work"
* Slightly relevant: "It helps in my work, but not for the core of it"
* Important: "It's important for the core of my work, but if it didn't exist, there are other packages or solutions to obtain something similar" 
* Essential: "I wouldn't have done the key part of my work without this package"

```{r importance, echo=FALSE, message=F, fig.width=16, fig.height=15, fig.cap="Figure 5. Bar plots of absolute frequency of each category of package relevance"}
data_question <- data_all[,grep("q3", colnames(data_all))]
colnames(data_question) <- t(packages)
# I'm dropping trajr that was added in the end and only got 1 response
data_question <- data_question[,-grep("trajr",colnames(data_question))]
packages_new <- packages[-which(packages=="trajr"),]

categories <- c("Not relevant","Slightly relevant","Important","Essential")
use_counts <- t(sapply(1:ncol(data_question),function(x){
  data_line <- factor(data_question[,x],levels=categories)
  count_p_use <- as.numeric(table(data_line))
  return(count_p_use)
}))
use_counts<-data.frame(use_counts)
colnames(use_counts) <- categories
rownames(use_counts) <- t(packages_new)
total_package <- rowSums(use_counts)
use_counts$package<-row.names(use_counts)
use_counts <- use_counts[total_package > 0,]

df1<-melt(use_counts,id.vars='package',variable_name='response')

g<-unlist(by(df1, df1$package, function(x) sum(x$value)))

color.pallete<-brewer.pal(5,'YlGnBu')
color.pallete[1]<-'lightgray'
df1$package<-factor(df1$package,levels=names(sort(g,decreasing=F)))

df1$response<-factor(df1$response,levels=rev(c('Essential','Important','Slightly relevant','Not relevant')))
ggplot(data=df1) + geom_col(aes(x=package, y=value,fill=response)) + 
  ylab('count') +coord_flip() + 
  scale_fill_manual(values=color.pallete) +theme_classic() +
  theme(axis.text.y = element_text( hjust = 1,vjust=0.5),
        axis.title =element_text(size=24),
        axis.text =element_text(size=16),
        legend.text=element_text(size=16),
        legend.title=element_blank())

```
<!---<span class="image-caption">caption comments</span>
-->

```{r importance counts,  echo=FALSE}
df2<-use_counts[,c("Not relevant","Slightly relevant","Important","Essential")]

use_per<-df2/apply(df2,1,sum)*100
use_per$good_excellent<-signif(apply(use_per[,c('Important','Essential')],1,sum),4)
use_per$counts<-apply(df2[,c('Important','Essential')],1,sum)
```
The two barplots show the absolute and relative frequency of the answers for each package, respectively.
We identified the packages that were highly relevant for their users, considering only those packages with at least 10 responses. Among these 33 packages, three were regarded as either "Important" or "Essential" for more than 75% of their users: *bsam* (`r use_per['bsam','good_excellent']`%; `r use_per['bsam','counts']`), *adehabitatHR* (`r use_per['adehabitatHR','good_excellent']`%; `r use_per['adehabitatHR','counts']`), and *adehabitatLT* (`r use_per['adehabitatLT','good_excellent']`%; `r use_per['adehabitatLT','counts']`). *bsam* allows fitting Bayesian state-space models to animal tracking data. 

```{r importance percentage, echo=FALSE, message=F,fig.width=16, fig.height=10, fig.cap="Figure 6. Bar plots of relative frequency of each category of package relevance (for packages with more than 5 users)"}

package_levels<-row.names(use_per[order(use_per$good_excellent,use_per$Essential, use_per$Important),])

use_per2<-use_per
use_per2[is.na(use_per2)]<-0
use_per2$package<-row.names(use_per2)
use_per2<-subset(use_per2,counts>5)
df1<-melt(subset(use_per2,select=-c(good_excellent,counts)),id.vars='package',variable_name='response')
df1$package<-factor(df1$package, levels=package_levels)
df1$response<-factor(df1$response,levels=c("Not relevant","Slightly relevant","Important","Essential"))
color.pallete<-brewer.pal(4,'YlGnBu')
color.pallete[1:2]<-c('lightgray','darkgray')
#color.pallete[1]<-c('lightgray')
ggplot(data=df1) + geom_col(aes(x=package, y=value,fill=response)) + 
  ylab('percentage') +coord_flip() + 
  scale_fill_manual(values=color.pallete) +theme_classic() +
  theme(axis.text.y = element_text( hjust = 1,vjust=0.5),
        axis.title =element_text(size=24),
        axis.text =element_text(size=15),
        legend.text=element_text(size=16),
        legend.title=element_blank())
```

If you want to check the numbers for your favorite package, the complete table is [here](hyperlink)

<!--
```{r, echo=FALSE, message=F}
kable(use_counts[,1:length(categories)])%>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"))
```
--> 
## Survey representativity

To get a rough idea of how representative the survey was of the population of the package users, we compared the number of participants that used each package to the number of monthly downloads that each package has. 
<!-- If the number of users based on the survey is proportional to the number of downloads, we could assume that the sample is representative. -->

The number of downloads were calculated using the R package cran.stats. It calculates the number of independent downloads by each package (substracting downloads by dependencies) by day. It only works for downloads using Rstudio, and for packages on CRAN. So for the tracking packages on CRAN, we computed the average of downloads per month, from September 2017 to August 2018; less months were considered for packages that were less than one year old. 

There is no perfect match between the number of users and the number of downloads per package, but a correlation 0.85 for the 49 packages on CRAN, provides evidence of an overall good representation of the users of tracking packages in the survey. Moreover, most of the packages with very few users in the survey regardless of their relatively high download statistics were accelerometry packages for human patients, which would be revealing that through Twitter and emails we did not reach that subpopulation of users. 

A log-log plot for both metrics is shown in the figure below.  



```{r, eval=TRUE,echo=FALSE,message=FALSE,warning=FALSE,fig.width=16, fig.height=15}
data_question <- data_all[,grep("q1", colnames(data_all))]
colnames(data_question) <- t(packages)
# I'm dropping trajr that was added in the end and only got 1 response
data_question <- data_question[,-grep("trajr",colnames(data_question))]
packages_new <- packages[-which(packages=="trajr"),]
categories <- c("Never","Rarely","Sometimes","Often")
use_counts <- t(sapply(1:ncol(data_question),function(x){
  data_line <- factor(data_question[,x],levels=categories)
  count_p_use <- as.numeric(table(data_line))
  return(count_p_use)
}))
use_counts<-data.frame(use_counts)
colnames(use_counts) <- categories
rownames(use_counts) <- t(packages_new)
use_counts$Package<-row.names(use_counts)
use_counts <- use_counts %>% 
  mutate(users = Rarely + Sometimes + Often)

# 
# matrix_all <- read.csv("Survey_Processed.csv")
# names(matrix_all) <- c("package",names(matrix_all)[2:ncol(matrix_all)])

funciones <- read.csv("RmovementPackagesMetadata-20180904.csv")
matrix_fun <- left_join(use_counts,funciones)
matrix_fun <- matrix_fun[(!is.na(matrix_fun$monthly.downloads)),]
# discarding the packages we did not get users from:
# matrix_fun_2 <- matrix_fun[matrix_fun$users > 0,] # missing: "GGIR"             "nparACT"          "PhysicalActivity" "smam"
# matrix_fun_3 <- matrix_fun[matrix_fun$Use_Counts >= 10,]

ggplot(matrix_fun,aes(x=users, y=monthly.downloads)) +
  geom_point(size=5) +
  xlab('Number of users') + ylab("Monthly downloads") +
  geom_text_repel(label=matrix_fun$Package,  segment.size=0.6,force=3,segment.alpha=.5,hjust=0,box.padding=0.5,min.segment.length=.1,size=6, direction = "both") +
  scale_x_continuous(trans='log10')+
  scale_y_continuous(trans='log10') +theme_classic() +
  theme(axis.text.y = element_text( hjust = 1,vjust=0.5),
        axis.title =element_text(size=24),
        axis.text =element_text(size=15))
# cor(matrix_fun$users,matrix_fun$monthly.downloads,use="pairwise.complete.obs")
# cor(matrix_fun_2$users,matrix_fun_2$monthly.downloads,use="pairwise.complete.obs")
```

## Summary 

* Most packages had very few users among the participants. The vast landscape of packages could be leading users to: 1) rely on the "old" packages (adehabitat) that gather most functions for common analyses in movement and 2) search for other packages when doing other specific analyses. Moreover, many packages contain functions that other packages have implemented too (see more details in the [review](https://arxiv.org/abs/1901.05935)), so repetition could make users spread between packages. 

* After the adehabitat packages, several packages for modeling animal movement (*momentuHMM*, *moveHMM*, *crawl* and *ctmm*) showed to be very popular, which could be an indicator of an increase in research on movement models. 

* Few of the packages had remarkably good documentation (>75% of "good" or "excellent" documentation), and, on the other end of the spectrum, a couple of packages got less than 50% of "good" or "excellent" rates. 

* Most packages were relevant for the work of their users, which is a positive feature!

* More analyses on the survey in the next post. Stay tuned!
